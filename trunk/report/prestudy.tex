\section{Matematikken bak trackingen (Vegar)}
    \subsection{Beregning av hodeposisjon}
    Det finnes mange måter å beregne posisjon til et objekt i forhold til et annet.
    Valg av fremgangsmåte tas på bakgrunn av egenskaper som blant annet kostnad, presisjon, bevegelsesområde og oppdateringshastighet.

    For dette systemet er det ingen bestemte krav for presisjon(oppløsning) eller oppdateringshastighet. Likevel bør ikke systemet
    føles hakkete, det vil si at både presisjonen og oppdateringshastigheten bør være høy nok til å unngå at bevegelse resulterer i
    hakkete bevegelse eller etterslep i en eventuell 3d-scene.

    Da kostnad er en av prioriteringene til dette systemet er bruk av Nintendo Wiimote, en relativt billig kontroller med
    innebygd optisk sensor og tracking av infrarøde lys, vurdert. Nintendo Wiimote har en oppløsning på 786 432 piksler og en oppdateringshastighet 
    på ? bilder i sekundet\ref{Wiimote hacking}. Tidlige tester antyder at oppløsningen til Wiimote er i nedre sjikt av hva som er
    akseptabelt, i tillegg er bevegelsesområdet noe begrenset på grunn av en noe lav utsynsvinkel på 45 grader.

    På grunn av dette vil det også ses på støtte for bruk av kamera med vilkårlig oppdateringshastighet og presisjon.
    Spesifikt et Panasonic HD-kamera med ~2,1 megapiksler, 60 grader utsynsvinkel og 60 bilder i sekundet. En vidvinkellinse
    kan også monteres for å bedre utsynsvinkelen.

    Beregning av hodeposisjon vil, på grunn av valgene over, bestå av å bruke informasjon om pikselposisjon av infrarøde punkter
    i et todimensjonalt bilde. For å forenkle denne beregningen brukes vinkelmål istedet for piksler, og det antas derfor
    at det er mulig å omregne pikselmål til vinkler. Ved bruk av Wiimote er det antatt at det er et konstant antall radianer
    mellom hver piksel: $$rpp = \frac{\frac{\pi}{4}}{1024}$$.

    Det bør nevnes at motivasjonen ligger i å beregne hodeposisjonen, det vil si at interessen ligger i bevegelse langs tre akser,
    men ikke rotasjon om aksene. Det er ikke dermed slik at det ikke er mulig å beregne hoderotasjonen, eller at brukeren ikke har
    mulighet til å rotere hodet. Årsaken til at det er interessant å kun beregne hodeposisjon er på grunn av at det kun er disse
    parametrene som er nødvendig for å få riktig perspektivprojeksjon i en 3d-scene, dette blir forklart mer dyptgående i \ref{3d}.

    \subsection{En optisk sensor og to infrarøde punkter}
    Når en optisk sensor og to infrarøde punkter er brukt, enten en wiimote eller et kamera, er det antatt følgende.
    \begin{enumerate}
    \item To infrarøde punkter er alltid observerbare fra optisk sensor. 
    \item Disse punktene er montert på hver sin side av brukerens hode.
    \item Det er en kjent avstand $l$ mellom punktene. 
    \item Brukerens hode er alltid vendt vinkelrett på den rette linjen fra sensor til brukeren.
    \item Sensoren er plassert horisontalt i midten av skjermen, enten rett over eller rett under skjermen.
    \end{enumerate}
    Punkt fire medfører at brukeren ikke har mulighet til å rotere hodet om y-aksen relativt til den rette linjen mot skjermplanet, og
    brukeren har derfor fem frihetsgrader for bevegelse: side til side, opp og ned, inn mot og fra skjermen, rotasjon om z-aksen og rotasjon om x-aksen.

    Da gjelder de følgende matematiske forholdene. Se figur.
	    \begin{figure}[h]
	    \centering
	    \includegraphics[width=0.80\textwidth]{graphics/Figur_en_sensor.jpg}
	    \caption{Modell med en sensor}
	    \label{fig:En_sensor}
	    \end{figure}

    \begin{eqnarray}\label{definition}
    &r = \frac{l/2}{\tan( \theta/2 )}\\
    &z = r \cos( \alpha )\\
    &x = r \sin( \alpha )\\
    &y = a + r \sin( \beta )
    \end{eqnarray}
    Parameteren $a$ angir forflytning langs y-aksen er tatt med for å kunne sette wiimote over eller under skjermen.
    Det er ikke tatt med en forflytningsparameter for x-aksen, da det er antatt at sensor er posisjonert horisontalt i midten av skjermen.
    Merk at $\beta$ er avhengig av vinkelen sensoren står i, ved at den for eksempel peker litt nedover.
    Da er $\beta = \beta^\prime + b$, hvor $\beta^\prime$ er vinkel fra kameraets midtpunkt og $b$ må kalibreres når brukeren står i en kjent posisjon.

    Antagelsen om at brukerens hode alltid er vendt mot sensoren er en følge av at kun to infrarøde punkter
    utgjør beregningsgrunnlaget. Med tre infrarøde punkter kan man oppnå full bevegelsesfrihet, ved bruk
    av algoritmer som \ref{hmm} eller \ref{hmm2}. Disse løsningene er iterative, og gir ikke eksakt(men veldig nære) løsninger.
    Metoden beskrevet her, med antagelsen om at brukeren er vendt mot sensor, gir en eksakt løsning på lukket form.

    \subsection{To optiske sensorer}

    Når to optiske sensorer benyttes, er det antatt følgende.
    \begin{enumerate}
    \item Ett punkt, det samme punktet, er observerbart fra begge sensorene. 
    \item Punktet ligger så nært hodets rotasjonssentrum at differansen er neglisjerbar.
    \item Begge sensorene er plassert i samme høyde og med lik avstand $l/2$ fra midten av skjermen.
    \end{enumerate}

    Da gjelder følgende( Se figur \ref{fig:Figur_to_sensorer}).
	    \begin{figure}[h]
	    \centering
	    \includegraphics[width=0.80\textwidth]{graphics/Figur_2_sensorer.jpg}
	    \caption{Illustrasjon to sensorer}
	    \label{fig:Figur_to_sensorer}
	    \end{figure}
    \begin{eqnarray}\label{definition2}
    &\frac{z}{a} = \tan( \alpha )\\
    &\frac{z}{b} = \tan( \beta )\\
    &l = a + b = z(\frac{1}{tan( \alpha )} + \frac{1}{tan( \beta )})
    &z = \frac{l}{\frac{1}{\alpha} + \frac{1}{\beta}}\\
    &x = \frac{l}{2} - \frac{z}{\tan( \alpha )}\\
    &y = z\sin( \gamma )
    \end{eqnarray}

    Her er både $\alpha$, $\beta$ og $\gamma$ bestående av en sum av en offset og vinkelmål fra sensor, siden
    sensorene står fritt til å peke inn mot eller ut fra skjermen, og helle litt nedover. Merk dog at 
    det er antatt at begge sensorene har samme helling nedover, hvilket vil gjøre at $\gamma$ kan
    brukes fra ett av sensorene eller midles fra begge.

    \subsection{Tredimensjonal perspektivprojeksjon}
    Et bruksområde som er muliggjort ved hjelp av hodetracking er å lage en fysisk riktig perspektivprojeksjon
    i en virtuell tredimensjonal scene. En perspektivprojeksjon gjør at objekter langt unna tegnes
    mindre enn nære objekter.
    Dette gjøres gjennom to funksjoner i OpenGL. Disse funksjonene
    manipulerer matrisene i OpenGL som transformerer koordinatene i 3d-scenen.
    Den ene, gluLookAt, modifiserer modellmatrisen slik at det virtuelle kameraet eller øyet flyttes
    og betraktningsretningen endres.  Den andre, glFrustum, modifiserer projeksjonsmatrisen slik at
    man får en perspektivprojeksjon. Perspektivprojeksjonen bestemmes av dybden til nærplanet/klippeplanet
    samt koordinatene til et rektangel på nærplanet.

    I korte trekk flyttes "øyet" til hodeposisjonen, og betraktningsretningen settes til å være vinkelrett på
    skjermplanet, det vil si at det virtuelle kamerat/øyet ikke roterer(et vindu vil ikke rotere om man ser på det fra en annen vinkel).
    Så endres perspektivprojeksjonen, slik at hjørnekoordinatene på rektangelet på nærplanet sammenfaller med hjørnekoordinatene til
    et virtuelt grid-rom. 

    De følgende ligningene benyttes for å korrigere perspektivet(regne ut koordinatene til nærplanet) når øyet er flyttet til posisjonen x,y,z. Figur \ref{figur} viser denne sammenhengen.
	    \begin{figure}[h]
	    \centering
	    \includegraphics[width=0.80\textwidth]{graphics/Perspektivkorreksjon.jpg}
	    \caption{HDMI-kontakt (venstre) og kabel (høyre)}
	    \label{fig:hdmi}
	    \end{figure}
    \begin{eqnarray}
    &p_{min} = b-x\\
    &p_{max} = a-x
    \end{eqnarray}
    En vanlig verdi for a = 1/2, b=-1/2.
    For y-aksen gjelder figur og ligninger med x utbyttet med y.


\section{Tilkobling av kamera til PC}

	For å koble et kamera til en PC og lese dens videobilder trenger man både maskinvare og programvare. Maskinvaren gir den fysiske tilkoblingen mellom kameraet og PC-en samt gjør lavnivås logikk og behandling av dataene. Programvaren abstraherer bort maskinvaren og gir utviklere et brukbart grensesnitt for å lese og bruke videobildene. Av maskinvare har vi i prosjektet tenkt å bruke HDMI og et Intensity-kort for å koble kameraet til PC-en som kjører systemet vårt. Av programvare har vi tenkt å bruke DirectShow for å få lett tilgang til videobildene fra kameraet.
	
\section{HDMI og Intensity}

	HDMI er en standard for et fysisk grensesnitt for å sende og motta digital lyd og bilde. Apparater med HDMI-kontakt kan kobles til hverandre ved hjelp av en HDMI-kabel (se figur \ref{fig:hdmi}). HDMI er designet for å håndtere lyd og bilde med høy oppløsning, og kan sende opptil 10,2 gigabit per sekund.
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.60\textwidth]{graphics/hdmi.jpg}
	\caption{HDMI-kontakt (venstre) og kabel (høyre)}
	\label{fig:hdmi}
	\end{figure}
	
	Intensity er et HDMI capture card fra Blackmagic Design. Det er et stykke maskinvare som kan settes inn i en PC for å gjøre det mulig å koble enheter til denne PC-en gjennom HDMI. Kortet kobles til PCI-express-porten i PC-en og har to HDMI-kontaktpunkter: inngang og utgang (se figur \ref{fig:intensity}).
	
	For å kunne få ut bilde fra HD kameraet trengte vi HDMI input port som overfører data i en så stor hastighet slik at vi slipper å komprimere bildestrømmen. Vi valgte Blackmagic Intesity card som gir den beste og siste teknologien innen HDMI til Windows eller Mac OS. Intesity kortet har en HDMI inngang for HD kameraer som gir den høyeste kvalitet på bilde. HDMI kan lese ukomprimert bildestrøm direkte fra kameraet. Bildestrømmen er i 1080i (interlaced) det vil si at bilde har en oppløsning på 1920 x 1080 piksler. 1080 linjer i vertikal retning og 1920 linjer i horisontal retning. Interlaced betyr at kameraet tar opp bilder med et alternerende sett med linjer slik at oddetallslinjene og partallslinjene oppdateres annenhver gang. Det vil si at det trengs to oppdateringer for å skape et fullstendig bilde, man øker bildekvaliteten av et videosignal uten å øke båndbredden. Når bildet skal vises på skjerm utføres det en deinterlacing som er en teknikk for å konvertere interlaced bildestrøm til progressive bildestrøm. Hvis det er 1080p (progressive) trenger bildestrømmen kun å oppdateres en gang for å få et fullstendig bilde, at alle linjene vertikalt blir skannet ved en oppdatering. 
	 
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.60\textwidth]{graphics/intensity.jpg}
	\caption{Intensity fra Blackmagic Design}
	\label{fig:intensity}
	\end{figure}

\section{DirectShow}

	DirectShow er et rammeverk og programvaregrensesnitt for å håndtere multimedia på PC-er. Det er utviklet av Microsoft og gjør det mulig å spille av og ta opp media (typisk lyd og/eller bilde) gjennom et felles grensesnitt, og abstraherer bort maskinvaren. Bildene fra videokamera koblet til et Intensity-kort er tilgjengelig gjennom DirectShow. DirectShow er tilgjengelig gratis fra Microsofts nettsider.
	
\section{Infrarødt lys}

	Infrarød lys er elekektromagnetisk stråling av bølgelengder lengre enn bølgelengdene til synlig lys. Det synlige lyset har bølgelengder mindre enn 750nm. Ordet infra kommer fra latinsk og betyr under, og rød er den fargen innenfor det synlige spekteret som har den lengste bølgelengden. Det infrarøde lyset som ligger nærmest det synlige lys har mye av de samme egnskapene som det synlige lyset bortsett fra at det er usynlig for det mennesklige øyet. Infrarødtlys er derfor mye brukt til blant annet overvåkning og nattkamera, i tillegg til signaler i elektriske kontroller. Infrarød stråling med lengre bølgelengder er mer knyttet til varmeproduksjon og varmesensitive kameraer som er mye brukt i romfartsindustri og våpenindustri. Se figur \ref{fig:irled} for et eksempel på en LED som sender infrarødt lys.
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.33\textwidth]{graphics/irled.png}
	\caption{Infrarød LED}
	\label{fig:irled}
	\end{figure}

	I dette prosjektet er det en fordel å bruke infrarødt lys fordi et lyspunkt som er synlig for det mennesklige øyet kan være forstyrrende for brukeren og det kan blande seg med andre lyskilder i omgivelsene rundt som vil gjøre det vanskligere finne punktene vi ønsker under prosesseringsarbeidet. 
	
\section{Nintendo Wii og Wiimotes}

	Nintendo Wii er en spillkonsoll som ble lansert i 2006. I stedet for realistisk grafikk fokuserer den på alternative former for styring av spill. Wiimote er navnet på kontrolleren som brukes for Wii. Se figur \ref{fig:wiimote}. Det er en trådløs kontroller med infrarødt kamera og akselerasjonsmeter. Innebygd er det maskinvare som kalkulerer bildekoordinater til filmede infrarøde lys og sender disse via et trådløst nettverk (Bluetooth).
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{graphics/wiimote.png}
	\caption{Nintendo Wiimote}
	\label{fig:wiimote}
	\end{figure}

\section{Bruksområder}

	Spill er et typisk bruksområde for headtracking. Eksempler er bilspill hvor en kan kikke deg rundt i kabinen, flysimulatorer hvor en må flytte på hodet for å se hele cockpiten, og slossspill der en beveger hodet til karakteren en spiller. En annen anvendelse er å styre karakteren sin i Second Life som er en virtuell virkelighet som kan brukes til konferanser og møter.
	
	Visualisering av store datamengder som en grafisk fremstilling av et reservoar kan ta i bruk head tracking for å navigere seg rundt i reservoaret for å få et mer riktig bilde av dimensjoner og oppbygning. For å oppnå dette kan headtracking-prorgramvare integreres med cavelib, som er et grensesnitt for visualisering som brukes mye av petroleums-programvare.
	
	Andre eksempler på bruk er styre systemer, som å tegne med hodet. Og navigere i kart, når skjermen er for liten til å vise all informasjonen kan head tracking brukes til å tracke vinkelen inn på skjermen for å kunne presentere forskjellige deler av kartet. Dette kan sammenlignes med å se gjennom en rute ned på et stort virtuelt kart, slik at forskjellige deler av kartet kommer til syne ved forskjellig synsvinkel.	
